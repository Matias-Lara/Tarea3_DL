{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d96ac7",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es un Transformer y qué hace este mini-GPT?\n",
    "\n",
    "### Conceptos básicos\n",
    "\n",
    "Un **Transformer** es una arquitectura de red neuronal diseñada para procesar secuencias de texto. A diferencia de las redes recurrentes (RNN), los Transformers procesan todo el texto simultáneamente usando un mecanismo llamado **self-attention**.\n",
    "\n",
    "#### ¿Cómo funciona?\n",
    "\n",
    "1. **Tokens**: El texto se divide en unidades pequeñas llamadas \"tokens\" (pueden ser palabras o caracteres).\n",
    "\n",
    "2. **Embeddings** (¡Importante!): Cada token se convierte en un **vector numérico** de dimensión fija. Este proceso se llama \"embedding\". \n",
    "   - Los embeddings son representaciones aprendidas que capturan el significado de cada token.\n",
    "   - Por ejemplo, palabras similares tendrán embeddings similares.\n",
    "   - El modelo aprende estos embeddings durante el entrenamiento.\n",
    "\n",
    "3. **Self-Attention**: Este mecanismo permite que cada posición del texto \"mire\" a todas las otras posiciones para entender el contexto.\n",
    "   - Cuando el modelo procesa la palabra \"banco\", puede mirar las palabras cercanas (\"río\" vs \"dinero\") para entender el contexto.\n",
    "   - Cada palabra decide cuánta atención prestarle a las demás palabras.\n",
    "\n",
    "4. **Predicción**: El modelo predice cuál es el siguiente token más probable en la secuencia.\n",
    "\n",
    "### ¿Qué hace este mini-GPT específicamente?\n",
    "\n",
    "- Es una **versión pequeña** de modelos como ChatGPT\n",
    "- Aprende a predecir el **siguiente carácter** en una conversación\n",
    "- Se entrena con conversaciones reales en español\n",
    "- Después del entrenamiento, puede **generar texto nuevo** imitando el estilo de las conversaciones\n",
    "\n",
    "### Limitaciones\n",
    "\n",
    "- No es ChatGPT: es mucho más simple\n",
    "- Solo aprende el estilo del archivo de entrenamiento\n",
    "- No tiene conocimiento general del mundo\n",
    "- Genera texto basándose en patrones estadísticos, no en \"comprensión\" real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d08642f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:10:42.750803Z",
     "iopub.status.busy": "2025-11-20T03:10:42.750492Z",
     "iopub.status.idle": "2025-11-20T03:10:42.755987Z",
     "shell.execute_reply": "2025-11-20T03:10:42.755200Z",
     "shell.execute_reply.started": "2025-11-20T03:10:42.750780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Módulos importados exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Agregar el directorio gpt-trained al path de Python\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/input/gpt-trained/gpt-trained\")\n",
    "\n",
    "\n",
    "# Importar las funciones y clases necesarias\n",
    "from src.preprocess import get_infrequent_tokens, mask_tokens, drop_chars, custom_tokenizer\n",
    "from src.model import GPTLanguageModel\n",
    "from src.train import model_training\n",
    "from src.utils import encode, decode, get_vocab\n",
    "\n",
    "import torch\n",
    "import json\n",
    "\n",
    "print(\"Módulos importados exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d2598",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento del texto (vocabulario y tokens)\n",
    "\n",
    "Antes de entrenar el modelo, necesitamos convertir el texto en números que la red neuronal pueda procesar.\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "####  Token\n",
    "Un **token** es la unidad básica de texto. En este modelo, cada carácter es un token (tokenización a nivel de caracteres).\n",
    "\n",
    "Ejemplo:\n",
    "- Texto: `\"Hola\"`\n",
    "- Tokens: `['H', 'o', 'l', 'a']`\n",
    "\n",
    "####  Vocabulario\n",
    "El **vocabulario** es el conjunto de todos los tokens únicos que aparecen en el texto de entrenamiento.\n",
    "\n",
    "Ejemplo:\n",
    "- Si el texto es `\"hola mundo\"`\n",
    "- Vocabulario: `['h', 'o', 'l', 'a', ' ', 'm', 'u', 'n', 'd']`\n",
    "\n",
    "####  Embedding \n",
    "El **embedding** es la conversión de cada token en un vector numérico de tamaño fijo.\n",
    "\n",
    "Por ejemplo:\n",
    "- Token 'a' → índice 5 → embedding vector de 256 dimensiones: `[0.23, -0.45, 0.12, ..., 0.67]`\n",
    "- Token 'e' → índice 10 → embedding vector de 256 dimensiones: `[0.18, -0.52, 0.08, ..., 0.71]`\n",
    "\n",
    "**¿Por qué son importantes los embeddings?**\n",
    "- Transforman símbolos discretos (letras) en representaciones continuas que la red puede procesar\n",
    "- Los embeddings se **aprenden durante el entrenamiento**\n",
    "- Capturan similitudes semánticas: letras que aparecen en contextos similares tendrán embeddings parecidos\n",
    "\n",
    "####  Codificación (encoding)\n",
    "La función `encode_text()` convierte cada carácter en su índice correspondiente en el vocabulario.\n",
    "\n",
    "Ejemplo:\n",
    "- Vocabulario: `['a', 'b', 'c', 'h', 'l', 'o']`\n",
    "- Texto: `\"hola\"`\n",
    "- Encoding: `[3, 5, 4, 0]` (índices en el vocabulario)\n",
    "\n",
    "####  Dataset autoregresivo\n",
    "El dataset se construye de manera que el modelo aprenda a predecir el siguiente carácter:\n",
    "- Entrada (X): `\"hol\"`\n",
    "- Salida esperada (Y): `\"ola\"`\n",
    "\n",
    "El modelo ve `\"hol\"` y debe aprender a predecir `\"a\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cad7c661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:10:42.757428Z",
     "iopub.status.busy": "2025-11-20T03:10:42.757152Z",
     "iopub.status.idle": "2025-11-20T03:10:42.771702Z",
     "shell.execute_reply": "2025-11-20T03:10:42.771042Z",
     "shell.execute_reply.started": "2025-11-20T03:10:42.757411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: hola mundo\n",
      "Tokens: ['h', 'o', 'l', 'a', ' ', 'm', 'u', 'n', 'd', 'o']\n",
      "Vocabulario: [' ', 'a', 'd', 'h', 'l', 'm', 'n', 'o', 'u']\n",
      "Codificación: [3, 7, 4, 1, 0, 5, 8, 6, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "# Funciones de preprocesamiento del archivo preprocess.py\n",
    "\n",
    "# Esta función identifica tokens que aparecen menos de min_count veces\n",
    "# Sirve para eliminar caracteres raros o emojis poco frecuentes\n",
    "def get_infrequent_tokens_example(tokens, min_count):\n",
    "    \"\"\"\n",
    "    Identifica tokens que aparecen menos de un mínimo de veces.\n",
    "    \n",
    "    Ejemplo:\n",
    "    tokens = \"hola mundo hola\"\n",
    "    min_count = 2\n",
    "    Resultado: {'m', 'u', 'n', 'd'} (aparecen solo 1 vez)\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    counts = Counter(tokens)\n",
    "    infreq_tokens = set([k for k,v in counts.items() if v < min_count])\n",
    "    return infreq_tokens\n",
    "\n",
    "# Esta función reemplaza tokens raros por un token especial <UNK> (unknown)\n",
    "def mask_tokens_example(tokens, mask):\n",
    "    \"\"\"\n",
    "    Reemplaza tokens raros por <UNK> (unknown token).\n",
    "    \n",
    "    Esto reduce el tamaño del vocabulario y ayuda a generalizar.\n",
    "    \"\"\"\n",
    "    unknown_token = \"<UNK>\"\n",
    "    return [t.replace(t, unknown_token) if t in mask else t for t in tokens]\n",
    "\n",
    "# Esta función construye el vocabulario (lista de tokens únicos)\n",
    "def build_vocab_example(tokens):\n",
    "    \"\"\"\n",
    "    Construye el vocabulario: lista ordenada de tokens únicos.\n",
    "    \n",
    "    Ejemplo:\n",
    "    tokens = ['h', 'o', 'l', 'a', 'h', 'o', 'l', 'a']\n",
    "    Resultado: ['a', 'h', 'l', 'o']\n",
    "    \"\"\"\n",
    "    return sorted(list(set(tokens)))\n",
    "\n",
    "# Esta función codifica tokens a números usando el vocabulario\n",
    "def encode_text_example(tokens, vocab):\n",
    "    \"\"\"\n",
    "    Convierte cada token a su índice en el vocabulario.\n",
    "    \n",
    "    Ejemplo:\n",
    "    tokens = ['h', 'o', 'l', 'a']\n",
    "    vocab = ['a', 'h', 'l', 'o']\n",
    "    Resultado: [1, 3, 2, 0]\n",
    "    \"\"\"\n",
    "    token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    return [token_to_idx.get(token, 0) for token in tokens]\n",
    "\n",
    "# Ejemplo práctico\n",
    "texto_ejemplo = \"hola mundo\"\n",
    "tokens_ejemplo = list(texto_ejemplo)\n",
    "vocab_ejemplo = build_vocab_example(tokens_ejemplo)\n",
    "encoded_ejemplo = encode_text_example(tokens_ejemplo, vocab_ejemplo)\n",
    "\n",
    "print(\"Texto original:\", texto_ejemplo)\n",
    "print(\"Tokens:\", tokens_ejemplo)\n",
    "print(\"Vocabulario:\", vocab_ejemplo)\n",
    "print(\"Codificación:\", encoded_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b47fea",
   "metadata": {},
   "source": [
    "## 3. Arquitectura del Transformer (Mini-GPT)\n",
    "\n",
    "El modelo GPT está compuesto por varias capas que trabajan en secuencia. Aquí explicamos cada componente.\n",
    "\n",
    "###  Embedding Layer (Capa de Embeddings)\n",
    "\n",
    "**¿Qué hace?**\n",
    "Convierte cada token (número entero) en un vector denso de números reales.\n",
    "\n",
    "**¿Por qué es importante?**\n",
    "- Los números enteros (0, 1, 2...) no capturan relaciones semánticas\n",
    "- Los embeddings permiten que tokens similares tengan representaciones similares\n",
    "- Son **aprendidos** durante el entrenamiento, no predefinidos\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "# Si tenemos un vocabulario de 50 tokens y queremos embeddings de dimensión 256:\n",
    "embedding_layer = nn.Embedding(50, 256)\n",
    "\n",
    "# Token 5 se convierte en un vector de 256 números:\n",
    "token_5 = torch.tensor([5])\n",
    "embedding_5 = embedding_layer(token_5)  # shape: (1, 256)\n",
    "```\n",
    "\n",
    "###  Positional Encoding (Codificación Posicional)\n",
    "\n",
    "**¿Qué hace?**\n",
    "Añade información sobre la **posición** de cada token en la secuencia.\n",
    "\n",
    "**¿Por qué es necesario?**\n",
    "- El self-attention no tiene noción del orden de las palabras por sí solo\n",
    "- \"El perro muerde al hombre\" ≠ \"El hombre muerde al perro\"\n",
    "- Los positional embeddings permiten al modelo saber que el token está en la posición 0, 1, 2, etc.\n",
    "\n",
    "**Cómo funciona:**\n",
    "```python\n",
    "# Cada posición (0, 1, 2, ..., 31) tiene su propio embedding aprendido\n",
    "pos_embedding = nn.Embedding(32, 256)  # 32 posiciones, 256 dimensiones\n",
    "\n",
    "# Se suma al token embedding:\n",
    "x = token_embedding + pos_embedding\n",
    "```\n",
    "\n",
    "###  Self-Attention (Auto-Atención)\n",
    "\n",
    "**¿Qué hace?**\n",
    "Permite que cada token \"mire\" a todos los otros tokens del contexto para entender mejor su significado.\n",
    "\n",
    "**Explicación simple:**\n",
    "Imagina que estás leyendo la frase: *\"El gato subió al árbol porque tenía miedo del perro\"*\n",
    "\n",
    "Cuando el modelo procesa la palabra \"tenía\", el self-attention le permite:\n",
    "- Mirar hacia atrás y ver \"gato\" → entiende quién tenía miedo\n",
    "- Mirar \"perro\" → entiende la causa del miedo\n",
    "\n",
    "**Cómo funciona técnicamente:**\n",
    "1. Cada token genera 3 vectores: **Query (Q)**, **Key (K)**, **Value (V)**\n",
    "2. Se calcula una puntuación de atención: qué tan relevante es cada token para el token actual\n",
    "3. Se usa esa puntuación para crear un promedio ponderado de los valores\n",
    "\n",
    "**Importante:** En GPT usamos **causal attention** (atención causal):\n",
    "- Un token solo puede mirar a tokens anteriores, no futuros\n",
    "- Esto se llama \"masked attention\" y se logra con una máscara triangular inferior\n",
    "\n",
    "###  Multi-Head Attention (Atención Multi-Cabeza)\n",
    "\n",
    "En lugar de una sola capa de self-attention, usamos **múltiples en paralelo** (6 cabezas en este modelo).\n",
    "\n",
    "**Ventaja:**\n",
    "- Cada cabeza puede aprender a prestar atención a diferentes aspectos\n",
    "- Una cabeza podría enfocarse en sintaxis, otra en semántica, etc.\n",
    "\n",
    "###  Feed-Forward Network (Red Neuronal Feed-Forward)\n",
    "\n",
    "Después del self-attention, cada token pasa por una pequeña red neuronal:\n",
    "```python\n",
    "# Expande la dimensión 4x, aplica ReLU, y luego vuelve al tamaño original\n",
    "Linear(256 → 1024) → ReLU → Linear(1024 → 256)\n",
    "```\n",
    "\n",
    "Esto permite que el modelo aprenda transformaciones no lineales complejas.\n",
    "\n",
    "###  Bloque Transformer\n",
    "\n",
    "Un bloque Transformer combina:\n",
    "1. Layer Normalization\n",
    "2. Multi-Head Self-Attention\n",
    "3. Conexión residual (skip connection)\n",
    "4. Layer Normalization\n",
    "5. Feed-Forward Network\n",
    "6. Otra conexión residual\n",
    "\n",
    "**Este modelo usa 6 bloques apilados secuencialmente.**\n",
    "\n",
    "###  Proyección Final (Output Layer)\n",
    "\n",
    "La última capa convierte el embedding de 256 dimensiones en **probabilidades** sobre todo el vocabulario:\n",
    "\n",
    "```python\n",
    "# Si el vocabulario tiene 80 tokens:\n",
    "Linear(256 → 80) → Softmax\n",
    "```\n",
    "\n",
    "El modelo produce una distribución de probabilidad:\n",
    "- Token 'a': 0.25 (25% de probabilidad)\n",
    "- Token 'e': 0.15 (15% de probabilidad)\n",
    "- ...\n",
    "\n",
    "Y se elige el token con mayor probabilidad (o se muestrea según la distribución)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ce900a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:10:42.773241Z",
     "iopub.status.busy": "2025-11-20T03:10:42.772990Z",
     "iopub.status.idle": "2025-11-20T03:10:42.793838Z",
     "shell.execute_reply": "2025-11-20T03:10:42.793096Z",
     "shell.execute_reply.started": "2025-11-20T03:10:42.773224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo GPT definido exitosamente\n",
      "Configuración:\n",
      "  - Tamaño de contexto: 32 caracteres\n",
      "  - Dimensión de embeddings: 256\n",
      "  - Número de cabezas: 6\n",
      "  - Número de bloques: 6\n"
     ]
    }
   ],
   "source": [
    "# Código del modelo GPTLanguageModel (desde model.py)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "# CONFIGURACIÓN (estos valores vienen de config.py)\n",
    "block_size = 32      # Tamaño máximo de contexto (32 caracteres)\n",
    "embed_size = 256     # Dimensión de los embeddings\n",
    "n_heads = 6          # Número de cabezas de atención\n",
    "n_layer = 6          # Número de bloques Transformer\n",
    "dropout = 0.2        # Tasa de dropout para regularización\n",
    "\n",
    "# 1. HEAD: Una sola cabeza de self-attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Una cabeza de self-attention\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # Proyecciones lineales para Query, Key, Value\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch, Time-steps, Channels\n",
    "        \n",
    "        # Calcular Q, K, V\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        \n",
    "        # Calcular puntuaciones de atención: Q @ K^T / sqrt(d_k)\n",
    "        wei = q @ k.transpose(-2, -1)  # (B, T, T)\n",
    "        wei /= math.sqrt(k.shape[-1])  # Escalado\n",
    "        \n",
    "        # Aplicar máscara causal (no mirar al futuro)\n",
    "        tril = torch.tril(torch.ones(T, T))\n",
    "        wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)  # Normalizar a probabilidades\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Aplicar atención a los valores\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        out = wei @ v      # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 2. MULTI-HEAD ATTENTION: Múltiples cabezas en paralelo\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Múltiples cabezas de self-attention en paralelo\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        # Crear n_heads cabezas en paralelo\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        # Proyección final\n",
    "        self.linear = nn.Linear(n_heads * head_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ejecutar todas las cabezas en paralelo y concatenar\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 3. FEED-FORWARD: Red neuronal simple\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"Red feed-forward con expansión 4x\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),  # Expandir\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),  # Contraer\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# 4. BLOCK: Un bloque Transformer completo\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Un bloque Transformer: self-attention + feed-forward\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention()      # Self-attention\n",
    "        self.ffwd = FeedFoward()            # Feed-forward\n",
    "        self.ln1 = nn.LayerNorm(embed_size) # Layer norm 1\n",
    "        self.ln2 = nn.LayerNorm(embed_size) # Layer norm 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conexiones residuales + layer norm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 5. MODELO COMPLETO: GPTLanguageModel\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"Modelo GPT completo\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # EMBEDDINGS\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embed_size)\n",
    "        \n",
    "        # BLOQUES TRANSFORMER (apilados secuencialmente)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "        \n",
    "        # SALIDA\n",
    "        self.ln_output = nn.LayerNorm(embed_size)\n",
    "        self.linear_output = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "        # Inicializar pesos\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # EMBEDDINGS: token + posición\n",
    "        tok_emb = self.token_embedding(idx)           # (B, T, embed_size)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T)) # (T, embed_size)\n",
    "        x = tok_emb + pos_emb                         # (B, T, embed_size)\n",
    "        \n",
    "        # BLOQUES TRANSFORMER\n",
    "        x = self.blocks(x)                            # (B, T, embed_size)\n",
    "        \n",
    "        # SALIDA\n",
    "        x = self.ln_output(x)                         # (B, T, embed_size)\n",
    "        logits = self.linear_output(x)                # (B, T, vocab_size)\n",
    "        \n",
    "        # CALCULAR LOSS (si tenemos targets)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B*T, C)\n",
    "            targets_flat = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "print(\"Modelo GPT definido exitosamente\")\n",
    "print(f\"Configuración:\")\n",
    "print(f\"  - Tamaño de contexto: {block_size} caracteres\")\n",
    "print(f\"  - Dimensión de embeddings: {embed_size}\")\n",
    "print(f\"  - Número de cabezas: {n_heads}\")\n",
    "print(f\"  - Número de bloques: {n_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50696901",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo con texto en español\n",
    "\n",
    "### ¿Cómo se entrena un modelo de lenguaje?\n",
    "\n",
    "El entrenamiento de un modelo GPT sigue un esquema **autoregresivo**:\n",
    "\n",
    "#### Paso 1: Cargar el texto en español\n",
    "Se carga un archivo `.txt` con conversaciones reales en español (por ejemplo, `chat.txt`).\n",
    "\n",
    "#### Paso 2: Crear pares de entrada-salida\n",
    "Para cada secuencia de texto, creamos:\n",
    "- **X (entrada)**: Una secuencia de N caracteres\n",
    "- **Y (salida esperada)**: Los mismos caracteres desplazados 1 posición\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Texto: \"Hola mundo\"\n",
    "\n",
    "X: \"Hola mund\"  →  Y: \"ola mundo\"\n",
    "```\n",
    "\n",
    "El modelo ve `\"Hola mund\"` y debe aprender a predecir `\"o\"` como siguiente carácter.\n",
    "\n",
    "#### Paso 3: Forward pass (predicción)\n",
    "1. Se codifica X en índices del vocabulario\n",
    "2. Se pasa por los embeddings\n",
    "3. Se procesa con los bloques Transformer\n",
    "4. Se obtienen las probabilidades de cada token\n",
    "\n",
    "#### Paso 4: Calcular el error (loss)\n",
    "Se compara la predicción del modelo con Y usando **Cross-Entropy Loss**:\n",
    "- Si el modelo predice correctamente el siguiente carácter → loss bajo\n",
    "- Si predice mal → loss alto\n",
    "\n",
    "#### Paso 5: Backpropagation y actualización\n",
    "1. Se calculan los gradientes (derivadas del loss respecto a los parámetros)\n",
    "2. Se actualizan los pesos usando el optimizador AdamW\n",
    "3. El modelo aprende a predecir mejor el siguiente carácter\n",
    "\n",
    "#### Paso 6: Repetir\n",
    "Se repite este proceso miles de veces (iteraciones) con diferentes fragmentos del texto.\n",
    "\n",
    "### División train/validation\n",
    "- **90% del texto** → entrenamiento\n",
    "- **10% del texto** → validación (para monitorear el progreso)\n",
    "\n",
    "### Hiperparámetros importantes\n",
    "- **Batch size**: 32 (número de secuencias procesadas simultáneamente)\n",
    "- **Learning rate**: 3e-4 (qué tan grandes son los pasos de actualización)\n",
    "- **Max iterations**: 5000 (número total de iteraciones de entrenamiento)\n",
    "- **Eval interval**: Cada 500 iteraciones se evalúa el modelo en validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b664766d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:10:42.795290Z",
     "iopub.status.busy": "2025-11-20T03:10:42.795116Z",
     "iopub.status.idle": "2025-11-20T03:10:42.811159Z",
     "shell.execute_reply": "2025-11-20T03:10:42.810456Z",
     "shell.execute_reply.started": "2025-11-20T03:10:42.795277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de entrenamiento lista\n"
     ]
    }
   ],
   "source": [
    "# Código de entrenamiento (simplificado desde train.py)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "learn_rate = 3e-4\n",
    "max_iters = 3000\n",
    "eval_interval = 500\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    \"\"\"Genera un batch aleatorio de datos\"\"\"\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, data, eval_iters=200):\n",
    "    \"\"\"Estima el loss promedio en un conjunto de datos\"\"\"\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data, batch_size, block_size)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "def train_model(text_path, vocab_size):\n",
    "    \"\"\"Entrena el modelo GPT con texto en español\"\"\"\n",
    "    \n",
    "    # 1. Cargar y preprocesar texto\n",
    "    with open(text_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    print(f\"Longitud del texto: {len(text):,} caracteres\")\n",
    "    \n",
    "    # 2. Construir vocabulario\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    print(f\"Vocabulario: {vocab_size} caracteres únicos\")\n",
    "    print(f\"Primeros 50 caracteres del vocabulario: {''.join(chars[:50])}\")\n",
    "    \n",
    "    # 3. Crear mapeos char <-> int\n",
    "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    itos = {i: ch for i, ch in enumerate(chars)}\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    \n",
    "    # 4. Codificar todo el texto\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    print(f\"Datos codificados: {len(data):,} tokens\")\n",
    "    \n",
    "    # 5. Split train/val\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    print(f\"Train: {len(train_data):,} tokens\")\n",
    "    print(f\"Validation: {len(val_data):,} tokens\")\n",
    "    \n",
    "    # 6. Inicializar modelo\n",
    "    model = GPTLanguageModel(vocab_size)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nModelo inicializado con {n_params:,} parámetros\")\n",
    "    \n",
    "    # 7. Configurar optimizador\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    # 8. Loop de entrenamiento\n",
    "    print(f\"\\nIniciando entrenamiento por {max_iters} iteraciones...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "        # Evaluar cada eval_interval iteraciones\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            train_loss = estimate_loss(model, train_data)\n",
    "            val_loss = estimate_loss(model, val_data)\n",
    "            print(f\"Iter {iter:4d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Obtener batch\n",
    "        xb, yb = get_batch(train_data, batch_size, block_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(xb, yb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Entrenamiento completado!\")\n",
    "    \n",
    "    # 9. Guardar modelo\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab': chars,\n",
    "        'stoi': stoi,\n",
    "        'itos': itos\n",
    "    }, 'gpt_model_espanol.pt')\n",
    "    print(\"Modelo guardado en: gpt_model_espanol.pt\")\n",
    "    \n",
    "    return model, stoi, itos\n",
    "\n",
    "print(\"Función de entrenamiento lista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed2b80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:10:42.812195Z",
     "iopub.status.busy": "2025-11-20T03:10:42.811897Z",
     "iopub.status.idle": "2025-11-20T03:26:38.139368Z",
     "shell.execute_reply": "2025-11-20T03:26:38.138185Z",
     "shell.execute_reply.started": "2025-11-20T03:10:42.812172Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto: 8,512 caracteres\n",
      "Vocabulario: 77 caracteres únicos\n",
      "Primeros 50 caracteres del vocabulario: \n",
      " !\",-.0123456789:?ABCDEFGHIJLMNOPQRSTVWY[]abcdefg\n",
      "Datos codificados: 8,512 tokens\n",
      "Train: 7,660 tokens\n",
      "Validation: 852 tokens\n",
      "\n",
      "Modelo inicializado con 4,757,581 parámetros\n",
      "\n",
      "Iniciando entrenamiento por 3000 iteraciones...\n",
      "============================================================\n",
      "Iter    0 | train loss: 4.4337 | val loss: 4.4220\n",
      "Iter  500 | train loss: 0.6749 | val loss: 1.9581\n",
      "Iter 1000 | train loss: 0.4055 | val loss: 2.4193\n",
      "Iter 1500 | train loss: 0.3528 | val loss: 2.6068\n",
      "Iter 2000 | train loss: 0.3083 | val loss: 2.8044\n",
      "Iter 2500 | train loss: 0.2645 | val loss: 2.9338\n",
      "Iter 2999 | train loss: 0.2209 | val loss: 3.1124\n",
      "============================================================\n",
      "Entrenamiento completado!\n",
      "Modelo guardado en: gpt_model_espanol.pt\n"
     ]
    }
   ],
   "source": [
    "# ENTRENAR EL MODELO\n",
    "\n",
    "# Ruta al archivo de texto en español\n",
    "text_path = \"/kaggle/input/gpt-trained/gpt-trained/assets/input/chat.txt\"\n",
    "\n",
    "# Entrenar\n",
    "model, stoi, itos = train_model(text_path, vocab_size=100)  # vocab_size se calculará automáticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a237349",
   "metadata": {},
   "source": [
    "## 5. Generar texto en español\n",
    "\n",
    "Una vez entrenado el modelo, podemos usarlo para **generar texto nuevo**.\n",
    "\n",
    "### ¿Cómo funciona la generación?\n",
    "\n",
    "1. **Inicio**: Damos al modelo un texto semilla (seed), por ejemplo: `\"Hola\"`\n",
    "\n",
    "2. **Predicción**: El modelo predice el siguiente carácter más probable\n",
    "   - `\"Hola\"` → modelo predice `\" \"` (espacio)\n",
    "\n",
    "3. **Actualización**: Añadimos el carácter predicho al contexto\n",
    "   - Nuevo contexto: `\"Hola \"`\n",
    "\n",
    "4. **Repetición**: Repetimos el proceso\n",
    "   - `\"Hola \"` → predice `\"c\"`\n",
    "   - `\"Hola c\"` → predice `\"ó\"`\n",
    "   - `\"Hola có\"` → predice `\"m\"`\n",
    "   - Y así sucesivamente...\n",
    "\n",
    "5. **Parada**: Nos detenemos cuando:\n",
    "   - Se genera un token especial de fin `<END>`\n",
    "   - O llegamos al límite de caracteres generados\n",
    "\n",
    "### Muestreo (Sampling)\n",
    "\n",
    "En lugar de siempre elegir el token más probable, **muestreamos** de la distribución de probabilidad:\n",
    "- Permite más creatividad y variabilidad\n",
    "- Evita que el modelo repita siempre lo mismo\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Distribución del siguiente carácter:\n",
    "  'a': 40%\n",
    "  'e': 30%\n",
    "  'o': 20%\n",
    "  'i': 10%\n",
    "\n",
    "En lugar de siempre elegir 'a', muestreamos según estas probabilidades.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89c5434e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:26:38.141152Z",
     "iopub.status.busy": "2025-11-20T03:26:38.140861Z",
     "iopub.status.idle": "2025-11-20T03:26:38.148214Z",
     "shell.execute_reply": "2025-11-20T03:26:38.147521Z",
     "shell.execute_reply.started": "2025-11-20T03:26:38.141132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de generación lista\n"
     ]
    }
   ],
   "source": [
    "# Función para generar texto\n",
    "\n",
    "def generate_text(model, seed_text, max_new_tokens=200, stoi=None, itos=None):\n",
    "    \"\"\"\n",
    "    Genera texto nuevo a partir de un texto semilla.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo GPT entrenado\n",
    "        seed_text: Texto inicial (semilla)\n",
    "        max_new_tokens: Máximo de caracteres nuevos a generar\n",
    "        stoi: Diccionario char -> int\n",
    "        itos: Diccionario int -> char\n",
    "    \n",
    "    Returns:\n",
    "        Texto generado completo (semilla + generación)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Codificar texto semilla\n",
    "    encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    \n",
    "    # Convertir a tensor\n",
    "    context = torch.tensor(encode(seed_text), dtype=torch.long).unsqueeze(0)  # (1, T)\n",
    "    \n",
    "    # Generar tokens uno por uno\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context si es muy largo (máximo block_size)\n",
    "            context_crop = context[:, -block_size:]\n",
    "            \n",
    "            # Obtener predicciones\n",
    "            logits, _ = model(context_crop)\n",
    "            \n",
    "            # Enfocarse solo en el último paso de tiempo\n",
    "            logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            \n",
    "            # Aplicar softmax para obtener probabilidades\n",
    "            probs = F.softmax(logits, dim=-1)  # (1, vocab_size)\n",
    "            \n",
    "            # Muestrear de la distribución\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (1, 1)\n",
    "            \n",
    "            # Añadir al contexto\n",
    "            context = torch.cat((context, idx_next), dim=1)  # (1, T+1)\n",
    "    \n",
    "    # Decodificar y retornar\n",
    "    generated = decode(context[0].tolist())\n",
    "    return generated\n",
    "\n",
    "print(\"Función de generación lista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e9bee0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:26:38.149145Z",
     "iopub.status.busy": "2025-11-20T03:26:38.148932Z",
     "iopub.status.idle": "2025-11-20T03:26:42.677219Z",
     "shell.execute_reply": "2025-11-20T03:26:42.676394Z",
     "shell.execute_reply.started": "2025-11-20T03:26:38.149130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto semilla: 'Hola, ¿cómo estás?'\n",
      "\n",
      "Generando...\n",
      "\n",
      "============================================================\n",
      "Hola, ¿cómo estás?\n",
      "[12.03.18, 17:13:17] Tom: ¿Qué tal juegos de mesa después de la cena?\n",
      "[12.03.18, 17:11:54] Brokovski: Estoy planeando un viaje a Europa el próximo verano.\n",
      "[12.03.18, 18:19:34] Alice: ¡Europa suena increíble! ¿A dónde irás?\n",
      "[12.03.18, 18:21:11] Tom: Me encantaría escuchar tus planes!\n",
      "[12.03.18, 19:0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# GENERAR TEXTO NUEVO\n",
    "\n",
    "# Texto semilla en español\n",
    "seed_text = \"Hola, ¿cómo estás?\"\n",
    "\n",
    "print(f\"Texto semilla: '{seed_text}'\")\n",
    "print(\"\\nGenerando...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "generated = generate_text(model, seed_text, max_new_tokens=300, stoi=stoi, itos=itos)\n",
    "\n",
    "print(generated)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "880bf18f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T03:26:42.678328Z",
     "iopub.status.busy": "2025-11-20T03:26:42.678052Z",
     "iopub.status.idle": "2025-11-20T03:26:51.707698Z",
     "shell.execute_reply": "2025-11-20T03:26:51.706972Z",
     "shell.execute_reply.started": "2025-11-20T03:26:42.678304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Semilla: 'Buenos días'\n",
      "============================================================\n",
      "Buenos días.\n",
      "[12.03.18, 17:21:22] Alice: También deberíamos preparar una lista de reproducción.\n",
      "[12.03.18, 17:22:48] Paul: He estado pensando en unas vacaciones \n",
      "\n",
      "============================================================\n",
      "Semilla: '¿Qué tal?'\n",
      "============================================================\n",
      "¿Qué tal?\n",
      "[12.03.18, 16:16:22] Alice: ¡Suena como una noche divertida!\n",
      "[12.03.18, 16:53:53] Tom: Tendremos que encontrar una fecha que funcione para todos.\n",
      "[12\n",
      "\n",
      "============================================================\n",
      "Semilla: 'Me gusta'\n",
      "============================================================\n",
      "Me gustarémendación de canciones?\n",
      "[12.03.18, 17:27:57] Brokovski: Estoy planeando un viaje a Europa el próximo verano.\n",
      "[12.03.18, 18:19:34] Alice: ¡Entonces s\n",
      "\n",
      "============================================================\n",
      "Semilla: 'Hoy es'\n",
      "============================================================\n",
      "Hoy estado, ¡y fue épica!\n",
      "[12.03.18, 17:48:00] Alice: Viajar y explorar nuevos lugares son las mejores experiencias.\n",
      "[12.03.18, 18:50:17] Paul: ¡Absolutamen\n"
     ]
    }
   ],
   "source": [
    "# Probar con diferentes semillas\n",
    "\n",
    "seeds = [\n",
    "    \"Buenos días\",\n",
    "    \"¿Qué tal?\",\n",
    "    \"Me gusta\",\n",
    "    \"Hoy es\"\n",
    "]\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Semilla: '{seed}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    generated = generate_text(model, seed, max_new_tokens=150, stoi=stoi, itos=itos)\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733ec3d",
   "metadata": {},
   "source": [
    "## 6. Conclusiones y limitaciones\n",
    "\n",
    "###  Lo que aprendimos\n",
    "\n",
    "1. **Arquitectura Transformer**:\n",
    "   - Cómo funcionan los **embeddings** para convertir texto en vectores numéricos\n",
    "   - El mecanismo de **self-attention** que permite que cada token mire a otros\n",
    "   - La combinación de múltiples bloques para crear modelos profundos\n",
    "\n",
    "2. **Entrenamiento**:\n",
    "   - Cómo se preprocesa el texto (tokenización, vocabulario, codificación)\n",
    "   - El esquema autoregresivo: predecir el siguiente carácter\n",
    "   - Optimización con backpropagation y AdamW\n",
    "\n",
    "3. **Generación**:\n",
    "   - Cómo generar texto nuevo carácter por carácter\n",
    "   - Muestreo de la distribución de probabilidad\n",
    "\n",
    "###  Limitaciones importantes\n",
    "\n",
    "1. **Tamaño del modelo**:\n",
    "   - Este mini-GPT tiene ~1-2 millones de parámetros\n",
    "   - GPT-3 tiene 175 **billones** de parámetros\n",
    "   - Por eso nuestro modelo es mucho más limitado\n",
    "\n",
    "2. **Datos de entrenamiento**:\n",
    "   - Solo aprende del archivo `chat.txt`\n",
    "   - No tiene conocimiento general del mundo\n",
    "   - Solo puede imitar el estilo de ese archivo específico\n",
    "\n",
    "3. **Capacidad de contexto**:\n",
    "   - Solo puede \"recordar\" los últimos 32 caracteres\n",
    "   - GPT-4 puede procesar miles de tokens\n",
    "\n",
    "4. **Calidad del texto generado**:\n",
    "   - Puede ser inconsistente o sin sentido\n",
    "   - No entiende semántica profunda\n",
    "   - Solo sigue patrones estadísticos\n",
    "\n",
    "###  Conceptos clave para recordar\n",
    "\n",
    "- **Embeddings**: Representaciones vectoriales aprendidas de tokens\n",
    "- **Self-Attention**: Mecanismo que permite al modelo entender relaciones entre palabras\n",
    "- **Autoregresivo**: Predecir el siguiente token basándose en los anteriores\n",
    "- **Causal Masking**: No permitir que el modelo vea el futuro durante el entrenamiento\n",
    "- **Layer Normalization**: Estabiliza el entrenamiento\n",
    "- **Residual Connections**: Ayudan al flujo de gradientes en redes profundas"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8785112,
     "sourceId": 13798406,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
